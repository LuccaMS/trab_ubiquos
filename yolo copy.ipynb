{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(os.listdir('imgs_test'), key=lambda x: int(x.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load each image, convert it to a byte array, and append to the list\n",
    "for filename in filenames:\n",
    "    with open(f'imgs_test/{filename}', 'rb') as f:\n",
    "        imgs.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert byte data to an image\n",
    "def bytes_to_image(byte_data):\n",
    "    np_arr = np.frombuffer(byte_data, np.uint8)\n",
    "    image = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the entry/exit boundary line (e.g., a horizontal line at y=250)\n",
    "boundary_y = 50\n",
    "entry_count = 0\n",
    "exit_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_positions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_track(frames):\n",
    "    global entry_count, exit_count, previous_positions\n",
    "    \n",
    "    for byte_data in frames:\n",
    "        # Convert byte data to image\n",
    "        frame = bytes_to_image(byte_data)\n",
    "        \n",
    "        # Ensure the frame was converted successfully\n",
    "        if frame is None:\n",
    "            continue\n",
    "        \n",
    "        # Track objects in the frame\n",
    "        results = model.track(source=frame)\n",
    "        \n",
    "        # Process the tracking results (e.g., draw bounding boxes)\n",
    "        current_positions = {}\n",
    "        \n",
    "        for result in results:\n",
    "            for box in result.boxes:\n",
    "                confidence = box.conf[0].item()\n",
    "\n",
    "                if confidence < confidence_threshold:\n",
    "                    continue\n",
    "\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy.tolist()[0])\n",
    "                object_id = int(box.id[0])  # Assuming 'id' contains the unique ID\n",
    "                \n",
    "                # Calculate the center of the bounding box\n",
    "                center_y = (y1 + y2) // 2\n",
    "                \n",
    "                # Store the current position\n",
    "                current_positions[object_id] = center_y\n",
    "                \n",
    "                # Draw the bounding box on the frame\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                #cv2.putText(frame, f'ID: {object_id}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "                #cv2.putText(frame, f'{confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "        # Check for entries and exits\n",
    "        for object_id, center_y in current_positions.items():\n",
    "            if object_id in previous_positions:\n",
    "                previous_y = previous_positions[object_id]\n",
    "                \n",
    "                # Check if the object has crossed the boundary\n",
    "                if previous_y <= boundary_y < center_y:\n",
    "                    entry_count += 1\n",
    "                elif previous_y >= boundary_y > center_y:\n",
    "                    exit_count += 1\n",
    "        \n",
    "        # Update previous positions\n",
    "        previous_positions = current_positions\n",
    "        \n",
    "        # Draw the boundary line\n",
    "        cv2.line(frame, (0, boundary_y), (frame.shape[1], boundary_y), (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the counts with smaller font size\n",
    "        cv2.putText(frame, f'Entries: {entry_count}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "        cv2.putText(frame, f'Exits: {exit_count}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "    \n",
    "        # Resize the frame to 500x500 pixels\n",
    "        resized_frame = cv2.resize(frame, (500, 500))\n",
    "        \n",
    "        # Display the frame (optional, for visualization purposes)\n",
    "        cv2.imshow('Tracked Frame', resized_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 1 person, 1473.1ms\n",
      "Speed: 4.0ms preprocess, 1473.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1482.0ms\n",
      "Speed: 3.0ms preprocess, 1482.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1446.1ms\n",
      "Speed: 3.0ms preprocess, 1446.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1410.2ms\n",
      "Speed: 3.0ms preprocess, 1410.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1537.9ms\n",
      "Speed: 3.0ms preprocess, 1537.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1551.3ms\n",
      "Speed: 6.0ms preprocess, 1551.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1450.1ms\n",
      "Speed: 3.0ms preprocess, 1450.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1497.0ms\n",
      "Speed: 3.0ms preprocess, 1497.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 1400.3ms\n",
      "Speed: 3.0ms preprocess, 1400.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1431.2ms\n",
      "Speed: 3.0ms preprocess, 1431.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1451.1ms\n",
      "Speed: 3.0ms preprocess, 1451.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1388.3ms\n",
      "Speed: 3.0ms preprocess, 1388.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1408.2ms\n",
      "Speed: 3.0ms preprocess, 1408.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1416.2ms\n",
      "Speed: 4.0ms preprocess, 1416.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1447.1ms\n",
      "Speed: 4.0ms preprocess, 1447.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1347.4ms\n",
      "Speed: 4.0ms preprocess, 1347.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1425.2ms\n",
      "Speed: 3.0ms preprocess, 1425.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1471.1ms\n",
      "Speed: 3.0ms preprocess, 1471.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1419.2ms\n",
      "Speed: 4.0ms preprocess, 1419.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1399.3ms\n",
      "Speed: 2.0ms preprocess, 1399.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1391.3ms\n",
      "Speed: 3.0ms preprocess, 1391.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1436.2ms\n",
      "Speed: 3.0ms preprocess, 1436.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1450.1ms\n",
      "Speed: 2.0ms preprocess, 1450.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1417.2ms\n",
      "Speed: 3.0ms preprocess, 1417.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1444.1ms\n",
      "Speed: 4.0ms preprocess, 1444.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 persons, 1384.3ms\n",
      "Speed: 3.0ms preprocess, 1384.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1392.3ms\n",
      "Speed: 3.0ms preprocess, 1392.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1426.2ms\n",
      "Speed: 2.0ms preprocess, 1426.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1466.1ms\n",
      "Speed: 3.0ms preprocess, 1466.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1413.2ms\n",
      "Speed: 3.0ms preprocess, 1413.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1446.1ms\n",
      "Speed: 3.0ms preprocess, 1446.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1425.2ms\n",
      "Speed: 3.0ms preprocess, 1425.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1403.2ms\n",
      "Speed: 3.0ms preprocess, 1403.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1437.2ms\n",
      "Speed: 4.0ms preprocess, 1437.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1469.1ms\n",
      "Speed: 4.0ms preprocess, 1469.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 person, 1455.9ms\n",
      "Speed: 4.0ms preprocess, 1455.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "process_and_track(imgs)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
